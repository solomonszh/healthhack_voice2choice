{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"): \n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader, Docx2txtLoader, DirectoryLoader, UnstructuredWordDocumentLoader, UnstructuredExcelLoader, CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, SpacyTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_iris import IRISVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize Chroma client\n",
    "chromadb_client = chromadb.PersistentClient(path=\"./chroma_db\")  # Saves data persistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/6bwdvxdj6tz_d082n8rflq140000gn/T/ipykernel_93310/3985152200.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'demo'\n",
    "password = 'demo' \n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '1972' \n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "# Under the hood, this becomes a SQL table. CANNOT have '.' in the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader('data', glob='*.docx', loader_cls=Docx2txtLoader)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admintech_solomonsoh/.pyenv/versions/3.10/envs/iris-env/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "text_splitter = SpacyTextSplitter(chunk_size=400, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLLECTION_NAME = \"cancer_db\"\n",
    "# # This creates a persistent vector store (a SQL table). You should run this ONCE only\n",
    "# db = IRISVector.from_documents(\n",
    "#     embedding=embeddings,\n",
    "#     documents=docs,\n",
    "#     collection_name=COLLECTION_NAME,\n",
    "#     connection_string=CONNECTION_STRING,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Create (or get) a collection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m collection \u001b[38;5;241m=\u001b[39m chromadb_client\u001b[38;5;241m.\u001b[39mget_or_create_collection(name\u001b[38;5;241m=\u001b[39mCOLLECTION_NAME)\n\u001b[0;32m----> 4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Add documents to ChromaDB\u001b[39;00m\n\u001b[1;32m      6\u001b[0m collection\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m      7\u001b[0m     ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docs))],  \u001b[38;5;66;03m# Unique IDs for each document\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     documents\u001b[38;5;241m=\u001b[39mdocs,  \u001b[38;5;66;03m# Raw text data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings  \u001b[38;5;66;03m# Precomputed embeddings\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/envs/iris-env/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:671\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/envs/iris-env/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:474\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m001\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# See: https://github.com/openai/openai-python/\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m#      issues/418#issuecomment-1525939500\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# replace newlines, which can negatively affect performance.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 474\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Split tokens into chunks respecting the embedding_ctx_length\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ctx_length):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/envs/iris-env/lib/python3.10/site-packages/tiktoken/core.py:120\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[1;32m    119\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m \u001b[43m_special_token_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    121\u001b[0m         raise_disallowed_special_token(match\u001b[38;5;241m.\u001b[39mgroup())\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"cancer_db\"\n",
    "# Create (or get) a collection\n",
    "collection = chromadb_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "embeddings = embedding_model.embed_documents(docs)\n",
    "# Add documents to ChromaDB\n",
    "collection.add(\n",
    "    ids=[str(i) for i in range(len(docs))],  # Unique IDs for each document\n",
    "    documents=docs,  # Raw text data\n",
    "    embeddings=embeddings  # Precomputed embeddings\n",
    ")\n",
    "\n",
    "print(\"Documents successfully added to ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"cancer_db\"\n",
    "# Subsequent calls to reconnect to the database and make searches should use this.  \n",
    "db = IRISVector(\n",
    "    embedding_function=embeddings,\n",
    "    dimension=1536,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of docs in vector store: {len(db.get()['ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/s_test.txt\", \"r\", encoding='ISO-8859-1')\n",
    "# query = \"new technology\"\n",
    "scenario = f.read()\n",
    "\n",
    "f = open(\"data/knowledge.docx\", \"r\", encoding='ISO-8859-1')\n",
    "# query = \"new technology\"\n",
    "knowledge = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_with_score = db.similarity_search_with_score(scenario, 2)\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = embeddings.embed_query(scenario)\n",
    "res = db.similarity_search_by_vector(embedding_vector)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_res = ''\n",
    "for each_res in res:\n",
    "    full_res = full_res + '\\n\\n' +each_res.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \n",
    "                f\"\"\"\n",
    "                A medical doctor with domain knowledge in breast cancer after having trained with a wealth of knowledge in these topics: {knowledge}.\n",
    "                Augment your data with results from {full_res}\n",
    "                \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \n",
    "                f\"\"\"\n",
    "                Given patient's consultation with the doctor in this {scenario}, \n",
    "                recommend \n",
    "                1. the best course of treatment\n",
    "                2. provide justifications for the course\n",
    "                3. provide chain of thought to reach those justifications\n",
    "                4. highlight risks to patient\n",
    "                \"\"\"\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pp(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader('data/treatment_selection.csv')#, csv_args={'fieldnames':['']})\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"pictures_db\"\n",
    "# This creates a persistent vector store (a SQL table). You should run this ONCE only\n",
    "db1 = IRISVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_with_score = db1.similarity_search_with_score(completion.choices[0].message.content, 1)\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_chosen = doc.page_content.split('\\n')[-1].split(': ')[-1] + \".jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion = client.chat.completions.create(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"system\", \n",
    "#             \"content\": \"\"\"\n",
    "#             Factors For Lumpectomy:\n",
    "# Breast Conservation – Preserves the natural breast, which may be important for some women, though Mdm Ang has indicated she may be able to accept a flat chest.\n",
    "# Less Invasive Surgery – Typically, a shorter recovery time compared to a mastectomy.\n",
    "# Factors Against Lumpectomy:\n",
    "# Need for Frequent Follow-Ups – Requires post-surgical radiotherapy, which means multiple hospital visits, a significant concern for Mdm Ang since she finds it difficult to travel to the hospital frequently.\n",
    "# Risk of Second Surgery – If the lumpectomy does not achieve clear margins, a second surgery may be required, which Mdm Ang wants to avoid.\n",
    "# Overall Treatment Burden – The combination of surgery and radiotherapy means a longer treatment course, which may not be ideal given her preference for a one-time treatment.\n",
    "# Since Mdm Ang prioritizes minimizing hospital visits and avoiding the possibility of a second surgery, mastectomy without reconstruction aligns better with her needs.\n",
    "# \"\"\"},\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": f\"\"\"\n",
    "# 1) cannot accept mastectomy\n",
    "# 2) wants reconstruction but cannot accept implant - can accept tram or LD flap \n",
    "# 3) only wants lumpectomy ok for second surgery\n",
    "# 4) tumor too big but really only want breast conserving and considering oncoplastic surgery - accepting of a slightly longer scar to maintain symmetry of best \n",
    "# 5) big tumor but cannot accept mastectomy, discuss neoadjuvant chemotherapy- risks and benefits and agreeable for trial of nact before mastectomy\n",
    "# 6) cost concerns. \n",
    "# Deciding between Breast conserving but with radiotherapy versus mastectomy without recon \n",
    "# Recon too expensive\n",
    "# 7) concern about drain management and no caregiver - prefers fast recovery - lumpectomy \n",
    "# 8 ) doesn’t want radiotherapy strongly  - mastectomy the.\n",
    "#             .\"\"\"\n",
    "#         }\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# text = \"Your long document text here...\"\n",
    "\n",
    "# splitter = CharacterTextSplitter(\n",
    "#     separator=\"\\n\\n\",\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=200\n",
    "# )\n",
    "\n",
    "# chunks = splitter.split_text(text) #you can also split documents using split_documents\n",
    "\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# text = \"Your long document text here...\"\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=200,\n",
    "#     length_function=len\n",
    "# )\n",
    "\n",
    "# chunks = splitter.split_text(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
